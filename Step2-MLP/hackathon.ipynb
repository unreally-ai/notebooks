{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a32684fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc9e26e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataloader\n",
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, stance_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            stance_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (TfidfVectorizer): sklearn vectorizer\n",
    "        \"\"\"\n",
    "        self.stance_df = stance_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        # splits (train, test, validation)\n",
    "        self.train, self.test = train_test_split(self.stance_df, test_size=0.3, shuffle=True)\n",
    "        self.test, self.val = train_test_split(self.test, test_size=0.5, shuffle=False)\n",
    "        # split sizes\n",
    "        self.train_len = len(self.train)\n",
    "        self.test_len = len(self.test)\n",
    "        self.val_len = len(self.val)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train, self.train_len),\n",
    "                             'val': (self.val, self.val_len),\n",
    "                             'test': (self.test, self.test_len)}\n",
    "\n",
    "    # returns length of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.stance_df)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        primary entry\n",
    "        Args:\n",
    "            index (int): index to current data point\n",
    "        Returns:\n",
    "            dictionary holding data point feature (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self.stance_df.iloc[index]\n",
    "        x_data = \\\n",
    "            self._vectorizer.fit_transform(row[0])\n",
    "\n",
    "        y_target = \\\n",
    "            self._vectorizer.fit_transform(row[2])\n",
    "\n",
    "        return {'x_data': review_vector,\n",
    "                'y_target': rating_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f20ec28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    load dataset and vectorizer\n",
    "    \"\"\"\n",
    "    print('loading dataset...')\n",
    "    df = pd.read_excel('fake_bananas_structuredKopie_4.xlsx', index_col=None, header=None)\n",
    "    df = df.drop([0], axis=0)\n",
    "    df = df.drop([1], axis=1)\n",
    "    print(df.head())\n",
    "    # instantiate vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english')\n",
    "    \n",
    "    dataset = StanceDataset(df, tfidf_vectorizer)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5dc61afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n",
      "                                                   0         2  \\\n",
      "1  Tourist dubbed ‘Spider Man’ after spider burro...     agree   \n",
      "2  Giant 8ft 9in catfish weighing 19 stone caught...  disagree   \n",
      "3  Enormous 20-stone catfish caught with fishing ...     agree   \n",
      "4          ISIS Militants Allegedly Contracted Ebola     agree   \n",
      "5  Matt Taibbi on leave of absence from First Loo...  disagree   \n",
      "\n",
      "                                                   3  \n",
      "1  A small meteorite crashed into a wooded area i...  \n",
      "2  A small meteorite crashed into a wooded area i...  \n",
      "3  A small meteorite crashed into a wooded area i...  \n",
      "4  (NEWSER) – Wonder how long a Quarter Pounder w...  \n",
      "5  (NEWSER) – Wonder how long a Quarter Pounder w...  \n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d2ca07ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [96]\u001b[0m, in \u001b[0;36mStanceDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mprimary entry\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    dictionary holding data point feature (x_data) and label (y_target)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstance_df\u001b[38;5;241m.\u001b[39miloc[index]\n\u001b[1;32m     37\u001b[0m x_data \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m y_target \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(row[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_data\u001b[39m\u001b[38;5;124m'\u001b[39m: review_vector,\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_target\u001b[39m\u001b[38;5;124m'\u001b[39m: rating_index}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2074\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2075\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2076\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2077\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2078\u001b[0m )\n\u001b[0;32m-> 2079\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1317\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;66;03m# We intentionally don't call the transform method to make\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;66;03m# fit_transform overridable without unwanted side effects in\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;66;03m# TfidfVectorizer.\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1319\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
