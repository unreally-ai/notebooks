{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a32684fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d0bef21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3231, 4)\n"
     ]
    }
   ],
   "source": [
    "# test block to tinker with vectorizer, add to custom dataloader once finished\n",
    "df = pd.read_excel('fake_bananas_structuredKopie_4.xlsx', index_col=None, header=None)\n",
    "df = df.drop([0], axis=0)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "def vectorizer_init(dataset):\n",
    "    \n",
    "    x = dataset.iloc[:,0:1]\n",
    "    x2 = dataset.iloc[:,2:3]\n",
    "    # create array of all texts in dataset\n",
    "    data = [(x[0])[i+1] for i in range(len(x)-1)]\n",
    "    data += ((x[0])[i+1] for i in range(len(x)-1))\n",
    "    \n",
    "    # fit to dataset (creates dictionary)\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    tfidf.fit(data)\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cb88ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2857)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try test block (works, already added to dataloader class)\n",
    "v = vectorizer_init(df)\n",
    "test_input = ['weee']\n",
    "\n",
    "print(len(v.vocabulary_))\n",
    "v.transform(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bc9e26e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataloader class\n",
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, stance_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            stance_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (TfidfVectorizer): sklearn vectorizer\n",
    "        \"\"\"\n",
    "        self.data = stance_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        # splits (train, test, validation)\n",
    "        self.train, self.test = train_test_split(self.data, test_size=0.3, shuffle=True)\n",
    "        self.test, self.val = train_test_split(self.test, test_size=0.5, shuffle=False)\n",
    "        # split sizes\n",
    "        self.train_len = len(self.train)\n",
    "        self.test_len = len(self.test)\n",
    "        self.val_len = len(self.val)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train, self.train_len),\n",
    "                             'val': (self.val, self.val_len),\n",
    "                             'test': (self.test, self.test_len)}\n",
    "\n",
    "    # returns length of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        primary entry\n",
    "        Args:\n",
    "            index (int): index to current data point\n",
    "        Returns:\n",
    "            dictionary holding data point feature (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        # get text values from current row ------!!! CHANGE ACCORDING TO DF !!!! -----\n",
    "        claim = [self.data[0][index]]\n",
    "        body = [self.data[3][index]]\n",
    "        \n",
    "        # transform claim (headline/tweet) and target (body) to tfidf vector\n",
    "        x = self._vectorizer.transform(claim)\n",
    "        y = self._vectorizer.transform(body)\n",
    "        \n",
    "        # take cosine similary\n",
    "        cosim = cosine_similarity(x, y)\n",
    "        \n",
    "        # concat x-cosim-y to input vector\n",
    "        claim_df = pd.DataFrame(x.toarray()) \n",
    "        body_df = pd.DataFrame(y.toarray())\n",
    "        cosim_df = pd.DataFrame(cosim)\n",
    "        \n",
    "        tenk = (pd.concat([claim_df, cosim_df, body_df],axis=1)).to_numpy()\n",
    "        #tenk = tenk.to_numpy()\n",
    "        #tenk = torch.from_numpy(tenk)\n",
    "        \n",
    "        return tenk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f20ec28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    load dataset and vectorizer\n",
    "    \"\"\"\n",
    "    print('loading dataset...')\n",
    "    df = pd.read_excel('fake_bananas_structuredKopie_4.xlsx', index_col=None, header=None)\n",
    "    df = df.drop([0], axis=0)\n",
    "    print(df.head())\n",
    "    # instantiate vectorizer\n",
    "    tfidf_vectorizer = vectorizer_init(df)\n",
    "    \n",
    "    dataset = StanceDataset(df, tfidf_vectorizer)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5dc61afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n",
      "                                                   0  1         2  \\\n",
      "1  Tourist dubbed ‘Spider Man’ after spider burro...  0     agree   \n",
      "2  Giant 8ft 9in catfish weighing 19 stone caught...  0  disagree   \n",
      "3  Enormous 20-stone catfish caught with fishing ...  0     agree   \n",
      "4          ISIS Militants Allegedly Contracted Ebola  5     agree   \n",
      "5  Matt Taibbi on leave of absence from First Loo...  5  disagree   \n",
      "\n",
      "                                                   3  \n",
      "1  A small meteorite crashed into a wooded area i...  \n",
      "2  A small meteorite crashed into a wooded area i...  \n",
      "3  A small meteorite crashed into a wooded area i...  \n",
      "4  (NEWSER) – Wonder how long a Quarter Pounder w...  \n",
      "5  (NEWSER) – Wonder how long a Quarter Pounder w...  \n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d2ca07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5715,)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is what the train loop should do each line. \n",
    "The goal is that this returns the exactly 1001 sized vec, but the dict of our df is too smol :(\n",
    "\"\"\"\n",
    "\n",
    "item = dataset.__getitem__(1)\n",
    "print(item)\n",
    "item[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
