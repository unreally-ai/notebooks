{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32684fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1568dccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3231, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
       "      <td>0</td>\n",
       "      <td>agree</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
       "      <td>0</td>\n",
       "      <td>disagree</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enormous 20-stone catfish caught with fishing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>agree</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIS Militants Allegedly Contracted Ebola</td>\n",
       "      <td>5</td>\n",
       "      <td>agree</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Matt Taibbi on leave of absence from First Loo...</td>\n",
       "      <td>5</td>\n",
       "      <td>disagree</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1         2  \\\n",
       "1  Tourist dubbed ‘Spider Man’ after spider burro...  0     agree   \n",
       "2  Giant 8ft 9in catfish weighing 19 stone caught...  0  disagree   \n",
       "3  Enormous 20-stone catfish caught with fishing ...  0     agree   \n",
       "4          ISIS Militants Allegedly Contracted Ebola  5     agree   \n",
       "5  Matt Taibbi on leave of absence from First Loo...  5  disagree   \n",
       "\n",
       "                                                   3  \n",
       "1  A small meteorite crashed into a wooded area i...  \n",
       "2  A small meteorite crashed into a wooded area i...  \n",
       "3  A small meteorite crashed into a wooded area i...  \n",
       "4  (NEWSER) – Wonder how long a Quarter Pounder w...  \n",
       "5  (NEWSER) – Wonder how long a Quarter Pounder w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('fake_bananas_structuredKopie_4.xlsx', index_col=None, header=None)\n",
    "df = df.drop([0])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e01cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in string & returns a cleaned string of all non-stop-words\n",
    "def preprocess(text, lemmatizer = WordNetLemmatizer()):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    s = \"\"\n",
    "    for word in text.split():\n",
    "        if word not in sw:\n",
    "                s += (lemmatizer.lemmatize(word) + \" \")\n",
    "    return s\n",
    "\n",
    "# creates a vocabulary without stop words\n",
    "# gets called in StanceDataset().__init__\n",
    "def create_vocab(df):\n",
    "    print(\"creating vocabulary...\")\n",
    "    vocab = []\n",
    "    for i in range(len(df)-1):\n",
    "            vocab.append(preprocess(df[0][i+1]))\n",
    "            vocab.append(preprocess(df[3][i+1]))\n",
    "    vocab_df = pd.DataFrame(vocab)\n",
    "    \n",
    "    counter = Counter(\" \".join(vocab_df[0]).split()).most_common(5000)\n",
    "    counter_df = pd.DataFrame(counter)\n",
    "    return counter_df\n",
    "\n",
    "#for testing\n",
    "#v = create_vocab(df)\n",
    "#print(len(v))\n",
    "#print(v.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0bef21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block to tinker with vectorizer, add to custom dataloader once finished\n",
    "def fit_vectorizers(dataset):\n",
    "    print('fitting vectorizer...')\n",
    "    x = dataset.iloc[:,0:1]\n",
    "    x2 = dataset.iloc[:,2:3]\n",
    "    # create array of all texts in dataset\n",
    "    data = [(x[0])[i+1] for i in range(len(x)-1)] # headlines\n",
    "    data += ((x[0])[i+1] for i in range(len(x)-1)) # bodies\n",
    "\n",
    "    vocab = create_vocab(dataset)[0]\n",
    "    \n",
    "    # fit to dataset (creates dictionary)\n",
    "    bow = CountVectorizer(\n",
    "        vocabulary=vocab,\n",
    "        max_features=5000,\n",
    "    )\n",
    "    tfidf = TfidfVectorizer(\n",
    "        vocabulary=vocab,\n",
    "        max_features=5000,\n",
    "    )\n",
    "    bow.fit(data)\n",
    "    tfidf.fit(data)\n",
    "    print('vectorizer ready!')\n",
    "    return bow, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb88ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting vectorizer...\n",
      "creating vocabulary...\n",
      "vectorizer ready!\n",
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 5000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test block (works, already added to dataloader class)\n",
    "bow, tfidf = fit_vectorizers(df)\n",
    "test_input = ['weee']\n",
    "\n",
    "print(len(bow.vocabulary_))\n",
    "bow.transform(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bc9e26e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataloader class\n",
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, stance_df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            stance_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (TfidfVectorizer): sklearn vectorizer\n",
    "        \"\"\"\n",
    "        self.data = stance_df\n",
    "        #self._vocab = create_vocab(self.data)\n",
    "        \n",
    "        self._bow, self._tfidf = fit_vectorizers(self.data)\n",
    "        \n",
    "        # splits (train, test, validation)\n",
    "        self.train, self.test = train_test_split(self.data, test_size=0.3, shuffle=True)\n",
    "        self.test, self.val = train_test_split(self.test, test_size=0.5, shuffle=False)\n",
    "        # split sizes\n",
    "        self.train_len = len(self.train)\n",
    "        self.test_len = len(self.test)\n",
    "        self.val_len = len(self.val)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train, self.train_len),\n",
    "                             'val': (self.val, self.val_len),\n",
    "                             'test': (self.test, self.test_len)}\n",
    "\n",
    "    # returns length of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        primary entry\n",
    "        Args:\n",
    "            index (int): index to current data point\n",
    "        Returns:\n",
    "            dictionary holding data point feature (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        # get text values from current row ------!!! CHANGE ACCORDING TO DF !!!! -----\n",
    "        claim = [self.data[0][index]]\n",
    "        body = [self.data[3][index]]\n",
    "\n",
    "        \n",
    "        # transform claim (headline/tweet) and target (body) to bowTF vector\n",
    "        bow_x = self._bow.transform(claim)\n",
    "        bow_y = self._bow.transform(body)\n",
    "\n",
    "        # transform claim (headline/tweet) and target (body) to TFIDF vector\n",
    "        tfidf_x = self._tfidf.transform(claim)\n",
    "        tfidf_y = self._tfidf.transform(body)\n",
    "        \n",
    "        # take cosine similary of TFIDF vectors\n",
    "        cosim = cosine_similarity(tfidf_x, tfidf_y)\n",
    "        \n",
    "        # concat x-cosim-y to input vector\n",
    "        claim_df = pd.DataFrame(bow_x.toarray()) \n",
    "        body_df = pd.DataFrame(bow_y.toarray())\n",
    "        cosim_df = pd.DataFrame(cosim)\n",
    "        \n",
    "        tenk = (pd.concat([claim_df, cosim_df, body_df],axis=1)).to_numpy()\n",
    "        tenk = torch.from_numpy(tenk)\n",
    "        \n",
    "        return tenk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f20ec28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    load dataset and vectorizer\n",
    "    \"\"\"\n",
    "    print('loading dataset...')\n",
    "    df = pd.read_excel('fake_bananas_structuredKopie_4.xlsx', index_col=None, header=None)\n",
    "    df = df.drop([0], axis=0)\n",
    "    print('loaded!')\n",
    "    # instantiate vectorizer\n",
    "    #tfidf_vectorizer = vectorizer_init(df)\n",
    "    \n",
    "    dataset = StanceDataset(df)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5dc61afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n",
      "loaded!\n",
      "fitting vectorizer...\n",
      "creating vocabulary...\n",
      "vectorizer ready!\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d2ca07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is what the train loop should do each line. \n",
    "\"\"\"\n",
    "\n",
    "item = dataset.__getitem__(1)\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cdee13",
   "metadata": {},
   "source": [
    "## MLP setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c8b1421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a28a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this simulates the training loop.\n",
    "Problems\n",
    "    - strange key error still happens\n",
    "    - when dataset.train is passed into DataLoader() instead, the key error happens way sooner\n",
    "    (is this because it's simply smaller or because .train doesnt work correctly?)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i in iter(train_loader):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f17c676b12c14782c06c04004144efea31e641eee12305210fddc7cd25bf6ccc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
