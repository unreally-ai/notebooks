{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a32684fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1568dccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3231, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
       "      <td>0</td>\n",
       "      <td>agree</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
       "      <td>0</td>\n",
       "      <td>disagree</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enormous 20-stone catfish caught with fishing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>agree</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIS Militants Allegedly Contracted Ebola</td>\n",
       "      <td>5</td>\n",
       "      <td>agree</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Matt Taibbi on leave of absence from First Loo...</td>\n",
       "      <td>5</td>\n",
       "      <td>disagree</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1         2  \\\n",
       "1  Tourist dubbed ‘Spider Man’ after spider burro...  0     agree   \n",
       "2  Giant 8ft 9in catfish weighing 19 stone caught...  0  disagree   \n",
       "3  Enormous 20-stone catfish caught with fishing ...  0     agree   \n",
       "4          ISIS Militants Allegedly Contracted Ebola  5     agree   \n",
       "5  Matt Taibbi on leave of absence from First Loo...  5  disagree   \n",
       "\n",
       "                                                   3  \n",
       "1  A small meteorite crashed into a wooded area i...  \n",
       "2  A small meteorite crashed into a wooded area i...  \n",
       "3  A small meteorite crashed into a wooded area i...  \n",
       "4  (NEWSER) – Wonder how long a Quarter Pounder w...  \n",
       "5  (NEWSER) – Wonder how long a Quarter Pounder w...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('fake_bananas_structuredKopie_4.xlsx', index_col=None, header=None)\n",
    "df = df.drop([0])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9e01cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in string & returns a cleaned string of all non-stop-words\n",
    "def preprocess(text, lemmatizer = WordNetLemmatizer()):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    s = \"\"\n",
    "    for word in text.split():\n",
    "        if word not in sw:\n",
    "                s += (lemmatizer.lemmatize(word) + \" \")\n",
    "    return s\n",
    "\n",
    "# creates a vocabulary without stop words\n",
    "# gets called in StanceDataset().__init__\n",
    "def create_vocab(df):\n",
    "    print(\"creating vocabulary...\")\n",
    "    vocab = []\n",
    "    for i in range(len(df)-1):\n",
    "            vocab.append(preprocess(df[0][i+1]))\n",
    "            vocab.append(preprocess(df[3][i+1]))\n",
    "    vocab_df = pd.DataFrame(vocab)\n",
    "    \n",
    "    counter = Counter(\" \".join(vocab_df[0]).split()).most_common(5000)\n",
    "    counter_df = pd.DataFrame(counter)\n",
    "    return counter_df\n",
    "\n",
    "#for testing\n",
    "#v = create_vocab(df)\n",
    "#print(len(v))\n",
    "#print(v.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0bef21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block to tinker with vectorizer, add to custom dataloader once finished\n",
    "def fit_vectorizer(dataset):\n",
    "    print('fitting vectorizer...')\n",
    "    x = dataset.iloc[:,0:1]\n",
    "    x2 = dataset.iloc[:,2:3]\n",
    "    # create array of all texts in dataset\n",
    "    data = [(x[0])[i+1] for i in range(len(x)-1)]\n",
    "    data += ((x[0])[i+1] for i in range(len(x)-1))\n",
    "    \n",
    "    # fit to dataset (creates dictionary)\n",
    "    tfidf = TfidfVectorizer(\n",
    "        vocabulary=create_vocab(dataset)[0]\n",
    "    )\n",
    "    tfidf.fit(data)\n",
    "    print('vectorizer ready!')\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb88ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2857)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try test block (works, already added to dataloader class)\n",
    "v = vectorizer_init(df)\n",
    "test_input = ['weee']\n",
    "\n",
    "print(len(v.vocabulary_))\n",
    "v.transform(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc9e26e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataloader class\n",
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, stance_df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            stance_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (TfidfVectorizer): sklearn vectorizer\n",
    "        \"\"\"\n",
    "        self.data = stance_df\n",
    "        #self._vocab = create_vocab(self.data)\n",
    "        \n",
    "        self._vectorizer = fit_vectorizer(self.data)\n",
    "        \n",
    "        # splits (train, test, validation)\n",
    "        self.train, self.test = train_test_split(self.data, test_size=0.3, shuffle=True)\n",
    "        self.test, self.val = train_test_split(self.test, test_size=0.5, shuffle=False)\n",
    "        # split sizes\n",
    "        self.train_len = len(self.train)\n",
    "        self.test_len = len(self.test)\n",
    "        self.val_len = len(self.val)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train, self.train_len),\n",
    "                             'val': (self.val, self.val_len),\n",
    "                             'test': (self.test, self.test_len)}\n",
    "\n",
    "    # returns length of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        primary entry\n",
    "        Args:\n",
    "            index (int): index to current data point\n",
    "        Returns:\n",
    "            dictionary holding data point feature (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        \n",
    "        # get text values from current row ------!!! CHANGE ACCORDING TO DF !!!! -----\n",
    "        claim = [self.data[0][index]]\n",
    "        body = [self.data[3][index]]\n",
    "        \n",
    "        # transform claim (headline/tweet) and target (body) to bow vector\n",
    "        x = self._vectorizer.transform(claim)\n",
    "        y = self._vectorizer.transform(body)\n",
    "        \n",
    "        # take cosine similary of TFIDF vectors\n",
    "        cosim = cosine_similarity(x, y)\n",
    "        \n",
    "        # transform claim (headline/tweet) and target (body) to  vector\n",
    "        \n",
    "        # concat x-cosim-y to input vector\n",
    "        claim_df = pd.DataFrame(x.toarray()) \n",
    "        body_df = pd.DataFrame(y.toarray())\n",
    "        cosim_df = pd.DataFrame(cosim)\n",
    "        \n",
    "        tenk = (pd.concat([claim_df, cosim_df, body_df],axis=1)).to_numpy()\n",
    "        #tenk = tenk.to_numpy()\n",
    "        #tenk = torch.from_numpy(tenk)\n",
    "        \n",
    "        return tenk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f20ec28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    load dataset and vectorizer\n",
    "    \"\"\"\n",
    "    print('loading dataset...')\n",
    "    df = pd.read_excel('fake_bananas_structuredKopie_4.xlsx', index_col=None, header=None)\n",
    "    df = df.drop([0], axis=0)\n",
    "    print(df.head())\n",
    "    # instantiate vectorizer\n",
    "    #tfidf_vectorizer = vectorizer_init(df)\n",
    "    \n",
    "    dataset = StanceDataset(df)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5dc61afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n",
      "                                                   0  1         2  \\\n",
      "1  Tourist dubbed ‘Spider Man’ after spider burro...  0     agree   \n",
      "2  Giant 8ft 9in catfish weighing 19 stone caught...  0  disagree   \n",
      "3  Enormous 20-stone catfish caught with fishing ...  0     agree   \n",
      "4          ISIS Militants Allegedly Contracted Ebola  5     agree   \n",
      "5  Matt Taibbi on leave of absence from First Loo...  5  disagree   \n",
      "\n",
      "                                                   3  \n",
      "1  A small meteorite crashed into a wooded area i...  \n",
      "2  A small meteorite crashed into a wooded area i...  \n",
      "3  A small meteorite crashed into a wooded area i...  \n",
      "4  (NEWSER) – Wonder how long a Quarter Pounder w...  \n",
      "5  (NEWSER) – Wonder how long a Quarter Pounder w...  \n",
      "fitting vectorizer...\n",
      "creating vocabulary...\n",
      "vectorizer ready!\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2ca07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is what the train loop should do each line. \n",
    "The goal is that this returns the exactly 10001 sized vec, but the dict of our df is too smol :(\n",
    "\"\"\"\n",
    "\n",
    "item = dataset.__getitem__(1)\n",
    "print(item)\n",
    "len(item[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
